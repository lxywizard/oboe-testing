{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import torch\n",
    "import kernels\n",
    "import gplvm\n",
    "from utils import transform_forward, transform_backward\n",
    "import os\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "result_folder = 'results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# leave-one-out cross validation across midsize OpenML datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimental settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 40 # the rank for PCA initialization\n",
    "max_procs = 8 # maximum number of processes to be simultaneously executed\n",
    "Q = 20 \n",
    "batch_size = 50\n",
    "n_epochs = 300\n",
    "lr = 1e-7\n",
    "N_max = 1000\n",
    "bo_n_init = 5 # number of entries to observe at the beginning\n",
    "bo_n_iters = rank # number of entries to observe in the end\n",
    "save_checkpoint = False\n",
    "fn_checkpoint = None\n",
    "checkpoint_period = 50\n",
    "maxiter = 100 # maximum number of iterations in PMF training (Bayesian optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code for running PMF is adopted from the PMF GitHub repository at https://github.com/rsheth80/pmf-automl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "fn_data = 'accuracy_matrix.csv'\n",
    "fn_data_feats = 'metafeatures.csv'\n",
    "\n",
    "ind_common = np.loadtxt('ind_common.csv').astype(int).tolist()\n",
    "\n",
    "def ei(fm, fv, ybest, xi=0.01, eps=1e-12):\n",
    "\n",
    "    fsd = torch.sqrt(fv) + eps\n",
    "    gamma = (fm - ybest - xi)/fsd\n",
    "\n",
    "    return fsd * (torch.distributions.Normal(0,1).cdf(gamma) * gamma\n",
    "                  + torch.distributions.Normal(0,1).log_prob(gamma).exp())\n",
    "\n",
    "def init_l1(Ytrain, Ftrain, ftest, n_init=5):\n",
    "\n",
    "    dis = np.abs(Ftrain - ftest).sum(axis=1)\n",
    "    ix_closest = np.argsort(dis)[:n_init]\n",
    "    ix_nonnan_pipelines \\\n",
    "            = np.where(np.invert(np.isnan(Ytrain[:,ix_closest].sum(axis=1))))[0]\n",
    "    ranks = np.apply_along_axis(st.rankdata, 0,\n",
    "                                Ytrain[ix_nonnan_pipelines[:,None],ix_closest])\n",
    "    ave_pipeline_ranks = ranks.mean(axis=1)\n",
    "    ix_init = ix_nonnan_pipelines[np.argsort(ave_pipeline_ranks)[::-1]]\n",
    "\n",
    "    return ix_init[:n_init]\n",
    "\n",
    "class BO(gplvm.GP):\n",
    "\n",
    "    def __init__(self, dim, kernel, acq_func, **kwargs):\n",
    "        super(BO, self).__init__(dim, np.asarray([]), np.asarray([]), kernel,\n",
    "                                 **kwargs)\n",
    "\n",
    "        self.acq_func = acq_func\n",
    "        self.ybest = None\n",
    "        self.xbest = None\n",
    "\n",
    "    def add(self, xnew, ynew):\n",
    "\n",
    "        xnew_ = torch.tensor(xnew, dtype=self.X.dtype).reshape((1,-1))\n",
    "        self.X = torch.cat((self.X, xnew_))\n",
    "        ynew_ = torch.tensor([ynew], dtype=self.y.dtype)\n",
    "        self.y = torch.cat((self.y, ynew_))\n",
    "        if self.ybest is None or ynew_ > self.ybest:\n",
    "            self.ybest = ynew_\n",
    "            self.xbest = xnew_\n",
    "        self.N += 1\n",
    "\n",
    "    def next(self, Xcandidates):\n",
    "\n",
    "        if not self.N:\n",
    "            return torch.randperm(Xcandidates.size()[0])[0]\n",
    "\n",
    "        fmean, fvar = self.posterior(Xcandidates)\n",
    "        return torch.argmax(self.acq_func(fmean, fvar, self.ybest))\n",
    "\n",
    "\n",
    "\n",
    "def bo_search(m, bo_n_init, bo_n_iters, Ytrain, Ftrain, ftest, ytest,\n",
    "              do_print=False):\n",
    "    \"\"\"\n",
    "    initializes BO with L1 warm-start (using dataset features). returns a\n",
    "    numpy array of length bo_n_iters holding the best performance attained\n",
    "    so far per iteration (including initialization).\n",
    "\n",
    "    bo_n_iters includes initialization iterations, i.e., after warm-start, BO\n",
    "    will run for bo_n_iters - bo_n_init iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    preds = BO(m.dim, m.kernel, ei,\n",
    "                  variance=transform_forward(m.variance))\n",
    "    ix_evaled = []\n",
    "    ix_candidates = np.where(np.invert(np.isnan(ytest)))[0].tolist()\n",
    "    ybest_list = []\n",
    "\n",
    "    ix_init = init_l1(Ytrain, Ftrain, ftest).tolist()\n",
    "    for l in range(bo_n_init):\n",
    "        ix = ix_init[l]\n",
    "        if not np.isnan(ytest[ix]):\n",
    "            preds.add(m.X[ix], ytest[ix])\n",
    "            ix_evaled.append(ix)\n",
    "            ix_candidates.remove(ix)\n",
    "        yb = preds.ybest\n",
    "        if yb is None:\n",
    "            yb = np.nan\n",
    "        ybest_list.append(yb)\n",
    "\n",
    "        if do_print:\n",
    "            print('Iter: %d, %g [%d], Best: %g' % (l, ytest[ix], ix, yb))\n",
    "\n",
    "    for l in range(bo_n_init, bo_n_iters):\n",
    "        ix = ix_candidates[preds.next(m.X[ix_candidates])]\n",
    "        preds.add(m.X[ix], ytest[ix])\n",
    "        ix_evaled.append(ix)\n",
    "        ix_candidates.remove(ix)\n",
    "        ybest_list.append(preds.ybest)\n",
    "\n",
    "        if do_print:\n",
    "            print('Iter: %d, %g [%d], Best: %g' \\\n",
    "                                    % (l, ytest[ix], ix, preds.ybest))\n",
    "\n",
    "    return np.asarray(ybest_list)\n",
    "\n",
    "def train(m, optimizer, f_callback=None, f_stop=None):\n",
    "\n",
    "    it = 0\n",
    "    while True:\n",
    "\n",
    "        try:\n",
    "            t = time.time()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            nll = m()\n",
    "            nll.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            it += 1\n",
    "            t = time.time() - t\n",
    "\n",
    "            if f_callback is not None:\n",
    "                f_callback(m, nll, it, t)\n",
    "\n",
    "            # f_stop should not be a substantial portion of total iteration time\n",
    "            if f_stop is not None and f_stop(m, nll, it, t):\n",
    "                break\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_in_parallel(training_index, test_index):\n",
    "    ids_train = list(np.array(ind_common)[training_index])\n",
    "    ids_test = list(np.array(ind_common)[test_index])\n",
    "\n",
    "    df = pd.read_csv(fn_data, index_col=0, header=0)\n",
    "    dataset_ids = df.columns.tolist()\n",
    "    dataset_ids = [int(dataset_ids[i]) for i in range(len(dataset_ids))]\n",
    "    Y = df.values.astype(np.float64)\n",
    "\n",
    "    ix_train = [dataset_ids.index(i) for i in ids_train]\n",
    "    ix_test = [dataset_ids.index(i) for i in ids_test] # only accepts an integer ids_test\n",
    "\n",
    "    Ytrain = Y[:, ix_train]\n",
    "    Ytest = Y[:, ix_test]\n",
    "\n",
    "    df = pd.read_csv(fn_data_feats, index_col=0, header=0)\n",
    "    dataset_ids = df.index\n",
    "    dataset_ids = [int(dataset_ids[i]) for i in range(len(dataset_ids))]\n",
    "\n",
    "    ix_train = [dataset_ids.index(i) for i in ids_train]\n",
    "    ix_test = [dataset_ids.index(i) for i in ids_test]\n",
    "\n",
    "    Ftrain = df.values[ix_train, :]\n",
    "    Ftest = df.values[ix_test, :]\n",
    "\n",
    "    ix_init = init_l1(Ytrain, Ftrain, Ftest,n_init=5).tolist()\n",
    "\n",
    "\n",
    "    def f_stop(m, v, it, t):\n",
    "\n",
    "        if it >= maxiter-1:\n",
    "            print('maxiter (%d) reached' % maxiter)\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    varn_list = []\n",
    "    logpr_list = []\n",
    "    t_list = []\n",
    "    def f_callback(m, v, it, t):\n",
    "        varn_list.append(transform_forward(m.variance).item())\n",
    "        logpr_list.append(m().item()/m.D)\n",
    "        if it == 1:\n",
    "            t_list.append(t)\n",
    "        else:\n",
    "            t_list.append(t_list[-1] + t)\n",
    "\n",
    "        if save_checkpoint and not (it % checkpoint_period):\n",
    "            torch.save(m.state_dict(), fn_checkpoint + '_it%d.pt' % it)\n",
    "\n",
    "        print('it=%d, f=%g, varn=%g, t: %g'\n",
    "              % (it, logpr_list[-1], transform_forward(m.variance), t_list[-1]))\n",
    "\n",
    "    # create initial latent space with PCA, first imputing missing observations\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    X = PCA(Q).fit_transform(imp.fit(Ytrain).transform(Ytrain))\n",
    "\n",
    "    # define model\n",
    "    kernel = kernels.Add(kernels.RBF(Q, lengthscale=None), kernels.White(Q))\n",
    "    m = gplvm.GPLVM(Q, X, Ytrain, kernel, N_max=N_max, D_max=batch_size)\n",
    "    if save_checkpoint:\n",
    "        torch.save(m.state_dict(), fn_checkpoint + '_it%d.pt' % 0)\n",
    "\n",
    "    # optimize\n",
    "    print('training...')\n",
    "    optimizer = torch.optim.SGD(m.parameters(), lr=lr)\n",
    "    m = train(m, optimizer, f_callback=f_callback, f_stop=f_stop)\n",
    "    if save_checkpoint:\n",
    "        torch.save(m.state_dict(), fn_checkpoint + '_itFinal.pt')\n",
    "\n",
    "    # evaluate model and random baselines\n",
    "    print('evaluating...')\n",
    "    with torch.no_grad():\n",
    "        Ytest = Ytest.astype(np.float32)\n",
    "        regrets_automl = np.zeros((bo_n_iters, Ytest.shape[1]))\n",
    "\n",
    "    #iterate over datasets\n",
    "        for d in np.arange(Ytest.shape[1]):\n",
    "            ybest = np.nanmax(Ytest[:,d])\n",
    "            regrets_automl[:,d] = ybest - bo_search(m, bo_n_init, bo_n_iters,\n",
    "                                                    Ytrain, Ftrain, Ftest[d,:],\n",
    "                                                    Ytest[:,d], do_print=True)\n",
    "\n",
    "    regret = pd.DataFrame(regrets_automl[:, 0].reshape(1, -1), index=ids_test, columns=['regret after {} iterations'.format(iter) for iter in range(1, rank+1)])\n",
    "    tlist = pd.DataFrame(np.array(t_list).reshape(1, -1), index=ids_test, columns=[\"time after {} iterations\".format(iter) for iter in range(1, maxiter)])\n",
    "\n",
    "    return [regret, tlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run leave-one-out across datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = mp.Pool(max_procs)\n",
    "\n",
    "result = [p1.apply_async(train_and_test_in_parallel, args=[training_index, test_index]) for training_index, test_index in loo.split(ind_common)]\n",
    "\n",
    "p1.close()\n",
    "p1.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regret_all = pd.DataFrame(columns=['regret after {} iterations'.format(iter) for iter in range(1, rank+1)])\n",
    "tlist_all = pd.DataFrame(columns=[\"time after {} iterations\".format(iter) for iter in range(1, maxiter)])\n",
    "\n",
    "for item in result:\n",
    "    regret_all = regret_all.append(item.get()[0])\n",
    "    tlist_all = tlist_all.append(item.get()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optional: save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regret_all.to_csv(os.path.join(result_folder, 'regrets_all.csv'), index=True, header=True)\n",
    "# tlist_all.to_csv(os.path.join(result_folder, 'tlist_all.csv'), index=True, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
