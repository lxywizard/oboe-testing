{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as st\n",
    "import multiprocessing as mp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "automl_path = '../../../oboe/automl/'\n",
    "sys.path.append(automl_path)\n",
    "\n",
    "import linalg\n",
    "import convex_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indloc(indices, ind):\n",
    "    return np.where(np.array(indices)==ind)[0][0]\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "errmtx_df = pd.read_csv(os.path.join(automl_path, 'defaults/error_matrix.csv'), index_col=0, header=0)\n",
    "errmtx = errmtx_df.values\n",
    "runtime_df = pd.read_csv(os.path.join(automl_path, 'defaults/runtime_matrix.csv'), index_col=0, header=0)\n",
    "runtime = runtime_df.values\n",
    "dataset_sizes_df = pd.read_csv(os.path.join(automl_path, 'defaults/dataset_sizes.csv'), index_col=0, header=0)\n",
    "\n",
    "ind_errmtx = errmtx_df.index.astype(int)\n",
    "ind_metafeatures = pd.read_csv('../collect_pmf_performance/metafeatures.csv', index_col=0, header=0).index\n",
    "ind_common = list(set(ind_errmtx).intersection(set(ind_metafeatures)))\n",
    "\n",
    "errmtx_common_df = errmtx_df.loc[ind_common]\n",
    "errmtx_common = errmtx_common_df.values\n",
    "runtime_common_df = runtime_df.loc[ind_common]\n",
    "runtime_common = runtime_common_df.values\n",
    "errmtx_pred = np.zeros(errmtx_common.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard PCA to get latent features of datasets and models\n",
    "X_pca, Y_pca, _ = linalg.pca(errmtx, threshold=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = 'results'\n",
    "\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimental settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_rank = 4\n",
    "final_rank = 40\n",
    "pick_largest_v_opt = True\n",
    "scalarization = 'D'\n",
    "n_init = 5 # number of entries inferred by nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# without meta-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regret_all = pd.DataFrame(columns=['rank {}'.format(rank) for rank in range(initial_rank, final_rank+1)])\n",
    "errors_encountered_all = []\n",
    "\n",
    "for train_index, test_index in tqdm(loo.split(errmtx_common)):\n",
    "    \n",
    "    regret = []\n",
    "    new_row = np.zeros((1, errmtx_common.shape[1]))\n",
    "    # true best\n",
    "    y_best_true = min(errmtx_common[test_index[0], :])\n",
    "    # predicted best; initialize to be 1 (max achievable error)\n",
    "    y_best_pred = 1        \n",
    "    for rank in range(initial_rank, final_rank+1):\n",
    "        to_sample = linalg.pivot_columns(errmtx_common, rank=rank)\n",
    "        new_row[:, to_sample] = errmtx_common[test_index, to_sample]\n",
    "        errmtx_pred[test_index, :] = linalg.impute(errmtx_common, new_row, to_sample, rank=rank)\n",
    "        # predicted best\n",
    "        y_best_pred = min(y_best_pred, min(errmtx_common[test_index[0], to_sample]), errmtx_common[test_index[0], np.argmin(errmtx_pred[test_index[0], :])])\n",
    "        # collect regret\n",
    "        regret.append(y_best_pred - y_best_true)\n",
    "    regret_single = pd.DataFrame(np.array(regret).reshape(1, -1), index=[ind_common[test_index[0]]], columns=['rank {}'.format(rank) for rank in range(initial_rank, final_rank+1)])\n",
    "    regret_all = regret_all.append(regret_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "regret_all.to_csv(os.path.join(result_path, 'regrets_oboe_qr_incremental.csv'), index=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with metafeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_data_feats = 'metafeatures.csv'\n",
    "\n",
    "metafeatures_df = pd.read_csv(os.path.join('../collect_pmf_performance/', fn_data_feats), index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_l1(Ytrain, Ftrain, ftest, n_init=5):\n",
    "\n",
    "    dis = np.abs(Ftrain - ftest).sum(axis=1)\n",
    "    ix_closest = np.argsort(dis)[:n_init]\n",
    "    ix_nonnan_pipelines \\\n",
    "            = np.where(np.invert(np.isnan(Ytrain[:,ix_closest].sum(axis=1))))[0]\n",
    "    ranks = np.apply_along_axis(st.rankdata, 0,\n",
    "                                Ytrain[ix_nonnan_pipelines[:,None],ix_closest])\n",
    "    ave_pipeline_ranks = ranks.mean(axis=1)\n",
    "    ix_init = ix_nonnan_pipelines[np.argsort(ave_pipeline_ranks)[::-1]]\n",
    "\n",
    "    return ix_init[:n_init]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regret_all_with_mf = pd.DataFrame(columns=['rank {}'.format(rank) for rank in range(initial_rank, final_rank+1)])\n",
    "errors_encountered_all = []\n",
    "n_init = 5\n",
    "initial_rank = 5\n",
    "final_rank = 40\n",
    "\n",
    "for train_index, test_index in tqdm(loo.split(errmtx_common)):\n",
    "    try:\n",
    "        regret = []\n",
    "        new_row = np.zeros((1, errmtx_common.shape[1]))\n",
    "        # true best\n",
    "        y_best_true = min(errmtx_common[test_index[0], :])        \n",
    "        \n",
    "        Ftrain = metafeatures_df.loc[errmtx_common_df.index[train_index], :].values\n",
    "        Ftest = metafeatures_df.loc[errmtx_common_df.index[test_index], :].values\n",
    "        Ytrain = -errmtx_common_df.loc[errmtx_common_df.index[train_index], :].T.values\n",
    "        ix_init = init_l1(Ytrain, Ftrain, Ftest,n_init=n_init).tolist()\n",
    "        # predicted best\n",
    "        y_best_pred = min(errmtx_common[test_index[0], ix_init])\n",
    "        \n",
    "        for rank in range(initial_rank, final_rank+1):            \n",
    "            to_sample = list(set(linalg.pivot_columns(errmtx_common, rank=rank-n_init)).union(set(ix_init)))\n",
    "            new_row[:, to_sample] = errmtx_common[test_index, to_sample]\n",
    "            errmtx_pred[test_index, :] = linalg.impute(errmtx_common, new_row, to_sample, rank=rank)\n",
    "            # predicted best\n",
    "            y_best_pred = min(y_best_pred, min(errmtx_common[test_index[0], to_sample]), errmtx_common[test_index[0], np.argmin(errmtx_pred[test_index[0], :])])\n",
    "            # collect regret\n",
    "            regret.append(y_best_pred - y_best_true)\n",
    "        regret_single = pd.DataFrame(np.array(regret).reshape(1, -1), index=[ind_common[test_index[0]]], columns=['rank {}'.format(rank) for rank in range(initial_rank, final_rank+1)])\n",
    "        regret_all_with_mf = regret_all_with_mf.append(regret_single)\n",
    "    except:\n",
    "        print(\"error encountered on dataset {}\".format(ind_common[test_index[0]]))\n",
    "        errors_encountered_all.append(ind_common[test_index[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "regret_all_with_mf.to_csv(os.path.join(result_path, 'regrets_oboe_qr_incremental_with_mf.csv'), index=True, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
