{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as st\n",
    "import multiprocessing as mp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "automl_path = '../../../oboe/automl/'\n",
    "sys.path.append(automl_path)\n",
    "\n",
    "import linalg\n",
    "import convex_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indloc(indices, ind):\n",
    "    return np.where(np.array(indices)==ind)[0][0]\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "errmtx_df = pd.read_csv(os.path.join(automl_path, 'defaults/error_matrix.csv'), index_col=0, header=0)\n",
    "errmtx = errmtx_df.values\n",
    "runtime_df = pd.read_csv(os.path.join(automl_path, 'defaults/runtime_matrix.csv'), index_col=0, header=0)\n",
    "runtime = runtime_df.values\n",
    "dataset_sizes_df = pd.read_csv(os.path.join(automl_path, 'defaults/dataset_sizes.csv'), index_col=0, header=0)\n",
    "\n",
    "ind_errmtx = errmtx_df.index.astype(int)\n",
    "ind_metafeatures = pd.read_csv('../collect_pmf_performance/metafeatures.csv', index_col=0, header=0).index\n",
    "ind_common = list(set(ind_errmtx).intersection(set(ind_metafeatures)))\n",
    "\n",
    "errmtx_common_df = errmtx_df.loc[ind_common]\n",
    "errmtx_common = errmtx_common_df.values\n",
    "runtime_common_df = runtime_df.loc[ind_common]\n",
    "runtime_common = runtime_common_df.values\n",
    "errmtx_pred = np.zeros(errmtx_common.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard PCA to get latent features of datasets and models\n",
    "X_pca, Y_pca, _ = linalg.pca(errmtx, threshold=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = 'results'\n",
    "\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimental settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_rank = 4\n",
    "final_rank = 40\n",
    "pick_largest_v_opt = True\n",
    "scalarization = 'D'\n",
    "n_init = 5 # number of entries inferred by nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# number of entries constrained, without meta-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_entries_solve(N, Y, scalarization='D'):\n",
    "    n = Y.shape[1]\n",
    "    # It is observed the scipy.optimize solver in this problem usually converges within 50 iterations. Thus a maximum of 50 step is set as limit.\n",
    "    if scalarization == 'D':\n",
    "        def objective(v):\n",
    "            sign, log_det = np.linalg.slogdet(Y @ np.diag(v) @ Y.T)\n",
    "            return -1 * sign * log_det\n",
    "    elif scalarization == 'A':\n",
    "        def objective(v):\n",
    "            return np.trace(np.linalg.pinv(Y @ np.diag(v) @ Y.T))\n",
    "    elif scalarization == 'E':\n",
    "        def objective(v):\n",
    "            return np.linalg.norm(np.linalg.pinv(Y @ np.diag(v) @ Y.T), ord=2)\n",
    "    def constraint(v):\n",
    "        return N - np.sum(v)\n",
    "    v0 = np.full((n, ), 0.5)\n",
    "    constraints = {'type': 'ineq', 'fun': constraint}\n",
    "    v_opt = minimize(objective, v0, method='SLSQP', bounds=[(0, 1)] * n, options={'maxiter': 50},\n",
    "                     constraints=constraints)\n",
    "    return v_opt.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_largest_v_opt = True\n",
    "\n",
    "regret_all = pd.DataFrame(columns=['rank {}'.format(rank) for rank in range(initial_rank, final_rank+1)])\n",
    "errors_encountered_all = []\n",
    "\n",
    "for train_index, test_index in tqdm(loo.split(errmtx_common)):\n",
    "    \n",
    "    regret = []\n",
    "    new_row = np.zeros((1, errmtx_common.shape[1]))\n",
    "\n",
    "    # true best\n",
    "    y_best_true = min(errmtx_common[test_index[0], :])\n",
    "    # predicted best; initialize to be 1 (max achievable error)\n",
    "    y_best_pred = 1\n",
    "    for rank in range(initial_rank, final_rank+1):\n",
    "        v_opt = number_of_entries_solve(rank, Y_pca, scalarization)\n",
    "        if pick_largest_v_opt:\n",
    "            to_sample = np.argsort(-v_opt)[:rank]\n",
    "        else:\n",
    "            to_sample = np.where(v_opt > 0.9)[0]\n",
    "        new_row[:, to_sample] = errmtx_common[test_index, to_sample]\n",
    "        errmtx_pred[test_index, :] = linalg.impute(errmtx_common, new_row, to_sample, rank=rank)\n",
    "        # predicted best; only update when the new best is better (i.e., has lower error)\n",
    "        y_best_pred = min(y_best_pred, min(errmtx_common[test_index[0], to_sample]), errmtx_common[test_index[0], np.argmin(errmtx_pred[test_index[0], :])])\n",
    "        # collect regret\n",
    "        regret.append(y_best_pred - y_best_true)\n",
    "    regret_single = pd.DataFrame(np.array(regret).reshape(1, -1), index=[ind_common[test_index[0]]], columns=['rank {}'.format(rank) for rank in range(initial_rank, final_rank+1)])\n",
    "    regret_all = regret_all.append(regret_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regret_all.to_csv(os.path.join(result_path, 'regrets_oboe_ed_incremental_number_constrained.csv'), index=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# number of entries constrained, with meta-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_data_feats = 'metafeatures.csv'\n",
    "\n",
    "metafeatures_df = pd.read_csv(os.path.join('../collect_pmf_performance/', fn_data_feats), index_col=0, header=0)\n",
    "\n",
    "def init_l1(Ytrain, Ftrain, ftest, n_init=5):\n",
    "\n",
    "    dis = np.abs(Ftrain - ftest).sum(axis=1)\n",
    "    ix_closest = np.argsort(dis)[:n_init]\n",
    "    ix_nonnan_pipelines \\\n",
    "            = np.where(np.invert(np.isnan(Ytrain[:,ix_closest].sum(axis=1))))[0]\n",
    "    ranks = np.apply_along_axis(st.rankdata, 0,\n",
    "                                Ytrain[ix_nonnan_pipelines[:,None],ix_closest])\n",
    "    ave_pipeline_ranks = ranks.mean(axis=1)\n",
    "    ix_init = ix_nonnan_pipelines[np.argsort(ave_pipeline_ranks)[::-1]]\n",
    "\n",
    "    return ix_init[:n_init]\n",
    "\n",
    "regret_all_with_mf = pd.DataFrame(columns=['rank {}'.format(rank) for rank in range(initial_rank, final_rank+1)])\n",
    "errors_encountered_all = []\n",
    "\n",
    "for train_index, test_index in tqdm(loo.split(errmtx_common)):\n",
    "    \n",
    "    Ftrain = metafeatures_df.loc[errmtx_common_df.index[train_index], :].values\n",
    "    Ftest = metafeatures_df.loc[errmtx_common_df.index[test_index], :].values\n",
    "    Ytrain = -errmtx_common_df.loc[errmtx_common_df.index[train_index], :].T.values\n",
    "    ix_init = init_l1(Ytrain, Ftrain, Ftest,n_init=n_init).tolist()\n",
    "\n",
    "    regret = []\n",
    "    new_row = np.zeros((1, errmtx_common.shape[1]))\n",
    "\n",
    "    # true best\n",
    "    y_best_true = min(errmtx_common[test_index[0], :])\n",
    "\n",
    "    # predicted best\n",
    "    y_best_pred = min(errmtx_common[test_index[0], ix_init])\n",
    "\n",
    "    for rank in range(initial_rank, final_rank+1):\n",
    "        print(rank)\n",
    "        v_opt = number_of_entries_solve(rank-n_init, Y_pca, scalarization)\n",
    "        if pick_largest_v_opt:\n",
    "            to_sample = np.argsort(-v_opt)[:(rank-n_init)]\n",
    "        else:\n",
    "            to_sample = np.where(v_opt > 0.9)[0]\n",
    "        to_sample = list(set(to_sample).union(set(ix_init)))\n",
    "        new_row[:, to_sample] = errmtx_common[test_index, to_sample]\n",
    "        errmtx_pred[test_index, :] = linalg.impute(errmtx_common, new_row, to_sample, rank=rank)\n",
    "        # predicted best; only update when the new best is better (i.e., has lower error)\n",
    "        y_best_pred = min(y_best_pred, min(errmtx_common[test_index[0], to_sample]), errmtx_common[test_index[0], np.argmin(errmtx_pred[test_index[0], :])])\n",
    "        # collect regret\n",
    "        regret.append(y_best_pred - y_best_true)\n",
    "    regret_single = pd.DataFrame(np.array(regret).reshape(1, -1), index=[ind_common[test_index[0]]], columns=['rank {}'.format(rank) for rank in range(initial_rank, final_rank+1)])\n",
    "    regret_all_with_mf = regret_all_with_mf.append(regret_single)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regret_all_with_mf.to_csv(os.path.join(result_path, 'regrets_oboe_ed_incremental_with_mf_number_constrained.csv'), index=True, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
